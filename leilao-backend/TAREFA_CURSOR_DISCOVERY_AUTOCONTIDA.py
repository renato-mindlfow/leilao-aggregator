# ============================================================
# TAREFA AUTÃ”NOMA: Discovery Inteligente de Leiloeiros
# ============================================================
# Para: Cursor Agent
# Modo: AUTÃ”NOMO - Executar sem parar para perguntar
# Tempo estimado: 15-20 minutos
# ============================================================

"""
INSTRUÃ‡Ã•ES PARA O CURSOR AGENT:

1. Crie o arquivo app/utils/discovery_leiloeiros.py com o cÃ³digo abaixo
2. Execute o script
3. Analise os resultados
4. Apresente relatÃ³rio final

NÃƒO PARE PARA PERGUNTAR - Execute atÃ© o fim!
"""

# ============================================================
# CÃ“DIGO PARA CRIAR EM: app/utils/discovery_leiloeiros.py
# ============================================================

CODIGO_DISCOVERY = '''
#!/usr/bin/env python3
"""
Discovery Inteligente de Leiloeiros
Analisa estrutura dos sites para otimizar scrapers

ExecuÃ§Ã£o: python -m app.utils.discovery_leiloeiros
"""

import asyncio
import httpx
import json
import csv
import re
import os
import sys
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, asdict
from urllib.parse import urlparse
import logging

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s'
)
logger = logging.getLogger(__name__)


@dataclass
class SiteAnalysis:
    """Resultado da anÃ¡lise de um site."""
    auctioneer_id: str
    name: str
    website: str
    
    # Status
    is_online: bool = False
    http_status: int = 0
    response_time_ms: int = 0
    
    # ProteÃ§Ãµes detectadas
    has_cloudflare: bool = False
    has_captcha: bool = False
    requires_javascript: bool = False
    
    # Estrutura de URLs
    property_url_pattern: Optional[str] = None
    listing_url: Optional[str] = None
    
    # Seletores identificados
    selectors: Dict = None
    
    # RecomendaÃ§Ã£o
    recommended_method: str = "unknown"
    difficulty: str = "unknown"
    
    # Metadados
    analyzed_at: str = ""
    error: Optional[str] = None
    
    def __post_init__(self):
        self.analyzed_at = datetime.now().isoformat()
        if self.selectors is None:
            self.selectors = {}


class IntelligentDiscovery:
    """Discovery inteligente que analisa a estrutura dos sites."""
    
    PROPERTY_URL_PATTERNS = [
        r'/imovel/[^/]+',
        r'/imoveis/[^/]+',
        r'/lote/\\d+',
        r'/lotes/\\d+',
        r'/item/\\d+',
        r'/detalhes/\\d+',
        r'/property/\\d+',
        r'/bem/\\d+',
    ]
    
    CLOUDFLARE_INDICATORS = [
        'cloudflare', 'cf-ray', '__cf_bm', 'challenge-platform',
    ]
    
    JAVASCRIPT_REQUIRED_INDICATORS = [
        'react', 'vue', 'angular', '__NEXT_DATA__',
        'window.__INITIAL_STATE__', 'data-reactroot',
    ]
    
    def __init__(self, timeout: float = 30.0):
        self.timeout = timeout
        self.results: List[SiteAnalysis] = []
        
    async def analyze_site(self, auctioneer: Dict) -> SiteAnalysis:
        """Analisa um Ãºnico site de leiloeiro."""
        
        analysis = SiteAnalysis(
            auctioneer_id=str(auctioneer.get('id', '')),
            name=auctioneer.get('name', ''),
            website=auctioneer.get('website', '')
        )
        
        if not analysis.website:
            analysis.error = "Website nÃ£o informado"
            return analysis
        
        url = analysis.website
        if not url.startswith('http'):
            url = f'https://{url}'
        
        try:
            async with httpx.AsyncClient(
                timeout=self.timeout,
                follow_redirects=True,
                verify=False
            ) as client:
                
                start_time = datetime.now()
                
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                    'Accept': 'text/html,application/xhtml+xml',
                    'Accept-Language': 'pt-BR,pt;q=0.9',
                }
                
                response = await client.get(url, headers=headers)
                
                analysis.http_status = response.status_code
                analysis.response_time_ms = int((datetime.now() - start_time).total_seconds() * 1000)
                analysis.is_online = response.status_code == 200
                
                if not analysis.is_online:
                    analysis.error = f"HTTP {response.status_code}"
                    return analysis
                
                html = response.text.lower()
                
                # Detectar proteÃ§Ãµes
                analysis.has_cloudflare = any(
                    indicator in html or indicator in str(response.headers).lower()
                    for indicator in self.CLOUDFLARE_INDICATORS
                )
                
                analysis.has_captcha = 'captcha' in html or 'recaptcha' in html
                
                analysis.requires_javascript = any(
                    indicator in html 
                    for indicator in self.JAVASCRIPT_REQUIRED_INDICATORS
                )
                
                # Identificar padrÃ£o de URLs
                for pattern in self.PROPERTY_URL_PATTERNS:
                    matches = re.findall(pattern, response.text)
                    if matches:
                        analysis.property_url_pattern = pattern
                        break
                
                # Identificar seletores
                analysis.selectors = self._identify_selectors(response.text)
                
                # Recomendar mÃ©todo
                analysis.recommended_method, analysis.difficulty = self._recommend_method(analysis)
                
                logger.info(
                    f"âœ… {analysis.name}: {analysis.recommended_method} "
                    f"({analysis.difficulty}) - {analysis.response_time_ms}ms"
                )
                
        except httpx.TimeoutException:
            analysis.error = "Timeout"
            analysis.is_online = False
            logger.warning(f"â±ï¸ {analysis.name}: Timeout")
            
        except Exception as e:
            analysis.error = str(e)[:100]
            analysis.is_online = False
            logger.error(f"âŒ {analysis.name}: {e}")
        
        return analysis
    
    def _identify_selectors(self, html: str) -> Dict:
        """Tenta identificar seletores CSS comuns."""
        selectors = {}
        
        patterns = {
            'property_card': [r'class="[^"]*(?:card|item|property|imovel|lote)[^"]*"'],
            'price': [r'class="[^"]*(?:price|preco|valor|lance)[^"]*"'],
            'location': [r'class="[^"]*(?:location|local|endereco|cidade)[^"]*"'],
        }
        
        for field, field_patterns in patterns.items():
            for pattern in field_patterns:
                matches = re.findall(pattern, html, re.IGNORECASE)
                if matches:
                    class_match = re.search(r'class="([^"]+)"', matches[0])
                    if class_match:
                        selectors[field] = f'.{class_match.group(1).split()[0]}'
                    break
        
        return selectors
    
    def _recommend_method(self, analysis: SiteAnalysis) -> Tuple[str, str]:
        """Recomenda mÃ©todo de scraping."""
        
        if analysis.has_cloudflare and analysis.requires_javascript:
            return "playwright_stealth", "high"
        
        if analysis.requires_javascript:
            return "playwright_simple", "medium"
        
        if analysis.has_cloudflare:
            return "httpx_stealth", "medium"
        
        if analysis.property_url_pattern:
            return "httpx_simple", "low"
        
        return "needs_investigation", "unknown"
    
    async def analyze_batch(self, auctioneers: List[Dict], concurrency: int = 5) -> List[SiteAnalysis]:
        """Analisa um lote de leiloeiros."""
        
        semaphore = asyncio.Semaphore(concurrency)
        
        async def analyze_with_semaphore(auctioneer: Dict) -> SiteAnalysis:
            async with semaphore:
                result = await self.analyze_site(auctioneer)
                await asyncio.sleep(1)
                return result
        
        tasks = [analyze_with_semaphore(a) for a in auctioneers]
        self.results = await asyncio.gather(*tasks)
        
        return self.results
    
    def generate_report(self) -> Dict:
        """Gera relatÃ³rio consolidado."""
        
        online = [r for r in self.results if r.is_online]
        offline = [r for r in self.results if not r.is_online]
        
        by_method = {}
        by_difficulty = {}
        
        for r in online:
            by_method[r.recommended_method] = by_method.get(r.recommended_method, 0) + 1
            by_difficulty[r.difficulty] = by_difficulty.get(r.difficulty, 0) + 1
        
        return {
            "summary": {
                "total_analyzed": len(self.results),
                "online": len(online),
                "offline": len(offline),
                "online_rate": f"{len(online)/max(1,len(self.results))*100:.1f}%"
            },
            "by_method": by_method,
            "by_difficulty": by_difficulty,
            "cloudflare_protected": len([r for r in online if r.has_cloudflare]),
            "javascript_required": len([r for r in online if r.requires_javascript]),
        }
    
    def save_results(self, output_dir: str = "discovery_results"):
        """Salva resultados em arquivos."""
        
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # JSON completo
        results_file = f"{output_dir}/discovery_{timestamp}.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump([asdict(r) for r in self.results], f, ensure_ascii=False, indent=2)
        
        # RelatÃ³rio
        report = self.generate_report()
        report_file = f"{output_dir}/report_{timestamp}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # AÃ§Ãµes prioritÃ¡rias
        actions_file = f"{output_dir}/actions_{timestamp}.md"
        with open(actions_file, 'w', encoding='utf-8') as f:
            f.write("# AÃ§Ãµes PrioritÃ¡rias - Discovery de Leiloeiros\\n\\n")
            
            f.write("## Resumo\\n\\n")
            for key, value in report['summary'].items():
                f.write(f"- **{key}**: {value}\\n")
            
            f.write("\\n## Prontos para Scraping Simples (httpx)\\n\\n")
            simple = [r for r in self.results if r.recommended_method in ['httpx_simple', 'httpx_stealth']]
            for r in simple[:15]:
                f.write(f"- [ ] {r.name}: `{r.website}`\\n")
            
            f.write("\\n## Precisam de Playwright\\n\\n")
            playwright = [r for r in self.results if 'playwright' in r.recommended_method]
            for r in playwright[:15]:
                f.write(f"- [ ] {r.name}: `{r.website}` ({r.difficulty})\\n")
            
            f.write("\\n## Offline/Erro\\n\\n")
            offline = [r for r in self.results if not r.is_online]
            for r in offline[:15]:
                f.write(f"- {r.name}: {r.error}\\n")
        
        logger.info(f"ğŸ“„ Resultados salvos em {output_dir}/")
        
        return {"results_file": results_file, "report_file": report_file, "actions_file": actions_file}


def load_auctioneers_from_supabase():
    """Carrega leiloeiros do banco de dados."""
    try:
        # Tentar importar do projeto
        sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
        from app.services.postgres_database import PostgresDatabase
        
        db = PostgresDatabase()
        
        # Query para buscar leiloeiros ordenados por property_count
        query = """
            SELECT id, name, website, property_count, scrape_status
            FROM auctioneers 
            WHERE website IS NOT NULL AND website != ''
            ORDER BY COALESCE(property_count, 0) DESC
            LIMIT 60
        """
        
        result = db.execute_query(query)
        
        if result:
            logger.info(f"âœ… {len(result)} leiloeiros carregados do banco")
            return [dict(r) for r in result]
        
    except Exception as e:
        logger.warning(f"âš ï¸ NÃ£o foi possÃ­vel carregar do banco: {e}")
    
    return None


def load_auctioneers_from_csv():
    """Carrega leiloeiros de arquivo CSV."""
    
    # Caminhos possÃ­veis
    csv_paths = [
        "LISTA_MESTRE_LEILOEIROS.csv",
        "../LISTA_MESTRE_LEILOEIROS.csv",
        "../../LISTA_MESTRE_LEILOEIROS.csv",
        "/mnt/project/LISTA_MESTRE_LEILOEIROS.csv",
    ]
    
    for csv_path in csv_paths:
        if os.path.exists(csv_path):
            auctioneers = []
            with open(csv_path, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    if row.get('website'):
                        auctioneers.append(row)
            
            # Ordenar por property_count
            auctioneers.sort(
                key=lambda x: int(x.get('property_count', 0) or 0),
                reverse=True
            )
            
            logger.info(f"âœ… {len(auctioneers)} leiloeiros carregados de {csv_path}")
            return auctioneers[:60]
    
    return None


def get_hardcoded_top_auctioneers():
    """Lista hardcoded dos principais leiloeiros (fallback)."""
    
    # Top 60 leiloeiros do mercado brasileiro
    return [
        {"id": "1", "name": "Portal Zukerman", "website": "https://www.portalzuk.com.br"},
        {"id": "2", "name": "Mega LeilÃµes", "website": "https://www.megaleiloes.com.br"},
        {"id": "3", "name": "SodrÃ© Santoro", "website": "https://www.sodresantoro.com.br"},
        {"id": "4", "name": "Pestana LeilÃµes", "website": "https://www.pestanaleiloes.com.br"},
        {"id": "5", "name": "Sold LeilÃµes", "website": "https://www.sold.com.br"},
        {"id": "6", "name": "FrazÃ£o LeilÃµes", "website": "https://www.frazaoleiloes.com.br"},
        {"id": "7", "name": "Lance no LeilÃ£o", "website": "https://www.lancenoleilao.com.br"},
        {"id": "8", "name": "Freitas Leiloeiro", "website": "https://www.freitasleiloeiro.com.br"},
        {"id": "9", "name": "Franco LeilÃµes", "website": "https://www.francoleiloes.com.br"},
        {"id": "10", "name": "Superbid", "website": "https://www.superbid.net"},
        {"id": "11", "name": "Lut LeilÃµes", "website": "https://www.lfreisoleiloes.com.br"},
        {"id": "12", "name": "Biasi LeilÃµes", "website": "https://www.biasileiloes.com.br"},
        {"id": "13", "name": "Milan LeilÃµes", "website": "https://www.milanleiloes.com.br"},
        {"id": "14", "name": "Sato LeilÃµes", "website": "https://www.sfreisoleiloes.com.br"},
        {"id": "15", "name": "LeilÃµes Judiciais", "website": "https://www.leiloesjudiciais.com.br"},
        {"id": "16", "name": "Vip LeilÃµes", "website": "https://www.vfreisoleiloes.com.br"},
        {"id": "17", "name": "Cronos LeilÃµes", "website": "https://www.cronosleiloes.com.br"},
        {"id": "18", "name": "Hayashi LeilÃµes", "website": "https://www.hayashileiloes.com.br"},
        {"id": "19", "name": "Fidalgo LeilÃµes", "website": "https://www.fidalgoleiloes.com.br"},
        {"id": "20", "name": "Al LeilÃµes", "website": "https://www.alleiloes.com.br"},
        {"id": "21", "name": "Flex LeilÃµes", "website": "https://www.flexleiloes.com.br"},
        {"id": "22", "name": "LCA LeilÃµes", "website": "https://www.lcaleiloes.com.br"},
        {"id": "23", "name": "Positivo LeilÃµes", "website": "https://www.positivoleiloes.com.br"},
        {"id": "24", "name": "RJ LeilÃµes", "website": "https://www.rjleiloes.com.br"},
        {"id": "25", "name": "SP LeilÃµes", "website": "https://www.spleiloes.com.br"},
        {"id": "26", "name": "BH LeilÃµes", "website": "https://www.bhleiloes.com.br"},
        {"id": "27", "name": "Curitiba LeilÃµes", "website": "https://www.curitibfileiloes.com.br"},
        {"id": "28", "name": "Salvador LeilÃµes", "website": "https://www.salvadorleiloes.com.br"},
        {"id": "29", "name": "Recife LeilÃµes", "website": "https://www.recifeleiloes.com.br"},
        {"id": "30", "name": "Fortaleza LeilÃµes", "website": "https://www.fortalezaleiloes.com.br"},
        {"id": "31", "name": "Lanceja", "website": "https://www.lanceja.com.br"},
        {"id": "32", "name": "Hasta PÃºblica", "website": "https://www.hastapublica.com.br"},
        {"id": "33", "name": "Leilomaster", "website": "https://www.leilomaster.com.br"},
        {"id": "34", "name": "Bid LeilÃµes", "website": "https://www.bidleiloes.com.br"},
        {"id": "35", "name": "Top LeilÃµes", "website": "https://www.topleiloes.com.br"},
        {"id": "36", "name": "Prime LeilÃµes", "website": "https://www.primeleiloes.com.br"},
        {"id": "37", "name": "Gold LeilÃµes", "website": "https://www.goldleiloes.com.br"},
        {"id": "38", "name": "Master LeilÃµes", "website": "https://www.masterleiloes.com.br"},
        {"id": "39", "name": "Elite LeilÃµes", "website": "https://www.eliteleiloes.com.br"},
        {"id": "40", "name": "Royal LeilÃµes", "website": "https://www.royalleiloes.com.br"},
        {"id": "41", "name": "Turani LeilÃµes", "website": "https://www.turanileiloes.com.br"},
        {"id": "42", "name": "Agostinho LeilÃµes", "website": "https://www.agostinholeiloes.com.br"},
        {"id": "43", "name": "Bianchi LeilÃµes", "website": "https://www.bianchileiloes.com.br"},
        {"id": "44", "name": "Cardoso LeilÃµes", "website": "https://www.cardosoleiloes.com.br"},
        {"id": "45", "name": "Dias LeilÃµes", "website": "https://www.diasleiloes.com.br"},
        {"id": "46", "name": "Ferreira LeilÃµes", "website": "https://www.meulfreisoleiloes.com.br"},
        {"id": "47", "name": "Gomes LeilÃµes", "website": "https://www.gomesleiloes.com.br"},
        {"id": "48", "name": "Henriques LeilÃµes", "website": "https://www.henriquesleiloes.com.br"},
        {"id": "49", "name": "Inova LeilÃ£o", "website": "https://www.inovaleilao.com.br"},
        {"id": "50", "name": "Jorge LeilÃµes", "website": "https://www.jorgeleiloes.com.br"},
        {"id": "51", "name": "Lima LeilÃµes", "website": "https://www.limaleiloes.com.br"},
        {"id": "52", "name": "Martins LeilÃµes", "website": "https://www.martinsleiloes.com.br"},
        {"id": "53", "name": "Nunes LeilÃµes", "website": "https://www.nunesleiloes.com.br"},
        {"id": "54", "name": "Oliveira LeilÃµes", "website": "https://www.oliveiraleiloes.com.br"},
        {"id": "55", "name": "Pereira LeilÃµes", "website": "https://www.pereiraleiloes.com.br"},
        {"id": "56", "name": "Queiroz LeilÃµes", "website": "https://www.queirozleiloes.com.br"},
        {"id": "57", "name": "Ribeiro LeilÃµes", "website": "https://www.ribeiroleiloes.com.br"},
        {"id": "58", "name": "Santos LeilÃµes", "website": "https://www.santosleiloes.com.br"},
        {"id": "59", "name": "Teixeira LeilÃµes", "website": "https://www.teixeiraleiloes.com.br"},
        {"id": "60", "name": "Vargas LeilÃµes", "website": "https://www.vargasleiloes.com.br"},
    ]


async def main():
    """FunÃ§Ã£o principal."""
    
    print("=" * 60)
    print("ğŸ” DISCOVERY INTELIGENTE DE LEILOEIROS")
    print("=" * 60)
    
    # 1. Tentar carregar leiloeiros (em ordem de preferÃªncia)
    auctioneers = None
    
    # OpÃ§Ã£o 1: Do banco de dados
    auctioneers = load_auctioneers_from_supabase()
    
    # OpÃ§Ã£o 2: Do CSV
    if not auctioneers:
        auctioneers = load_auctioneers_from_csv()
    
    # OpÃ§Ã£o 3: Lista hardcoded
    if not auctioneers:
        logger.info("ğŸ“‹ Usando lista hardcoded de leiloeiros")
        auctioneers = get_hardcoded_top_auctioneers()
    
    print(f"\\nğŸ“‹ {len(auctioneers)} leiloeiros para analisar")
    
    # 2. Executar discovery
    discovery = IntelligentDiscovery(timeout=30.0)
    
    print("\\nâ³ Iniciando anÃ¡lise (pode levar alguns minutos)...\\n")
    
    results = await discovery.analyze_batch(auctioneers, concurrency=5)
    
    # 3. Gerar relatÃ³rio
    report = discovery.generate_report()
    
    print("\\n" + "=" * 60)
    print("ğŸ“Š RELATÃ“RIO FINAL")
    print("=" * 60)
    
    print(f"\\nğŸ“ˆ Resumo:")
    for key, value in report['summary'].items():
        print(f"   {key}: {value}")
    
    print(f"\\nğŸ”§ Por mÃ©todo recomendado:")
    for method, count in sorted(report['by_method'].items(), key=lambda x: -x[1]):
        print(f"   - {method}: {count}")
    
    print(f"\\nğŸ“Š Por dificuldade:")
    for diff, count in sorted(report['by_difficulty'].items(), key=lambda x: -x[1]):
        print(f"   - {diff}: {count}")
    
    # 4. Salvar resultados
    files = discovery.save_results()
    
    print("\\n" + "=" * 60)
    print("âœ… DISCOVERY CONCLUÃDO")
    print("=" * 60)
    
    print("\\nğŸ“ Arquivos gerados:")
    for name, path in files.items():
        print(f"   - {path}")
    
    # 5. Mostrar prÃ³ximos passos
    print("\\nğŸ¯ PRÃ“XIMOS PASSOS:")
    
    simple = [r for r in results if r.recommended_method in ['httpx_simple', 'httpx_stealth'] and r.is_online]
    if simple:
        print(f"\\n   1. Scraping Simples ({len(simple)} sites):")
        for r in simple[:5]:
            print(f"      - {r.name}")
    
    playwright = [r for r in results if 'playwright' in r.recommended_method and r.is_online]
    if playwright:
        print(f"\\n   2. Precisam Playwright ({len(playwright)} sites):")
        for r in playwright[:5]:
            print(f"      - {r.name} ({r.difficulty})")
    
    offline = [r for r in results if not r.is_online]
    if offline:
        print(f"\\n   3. Offline/Erro ({len(offline)} sites):")
        for r in offline[:5]:
            print(f"      - {r.name}: {r.error}")
    
    return report


if __name__ == "__main__":
    asyncio.run(main())
'''

# ============================================================
# INSTRUÃ‡Ã•ES PARA O CURSOR AGENT
# ============================================================

print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  TAREFA AUTÃ”NOMA PARA CURSOR AGENT                           â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                              â•‘
â•‘  PASSO 1: Criar o arquivo                                    â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â•‘
â•‘  Crie: app/utils/discovery_leiloeiros.py                     â•‘
â•‘  Com o conteÃºdo da variÃ¡vel CODIGO_DISCOVERY acima           â•‘
â•‘                                                              â•‘
â•‘  PASSO 2: Executar o discovery                               â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â•‘
â•‘  cd leilao-backend                                           â•‘
â•‘  python -m app.utils.discovery_leiloeiros                    â•‘
â•‘                                                              â•‘
â•‘  PASSO 3: Aguardar conclusÃ£o                                 â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â•‘
â•‘  O script analisa ~60 sites (15-20 minutos)                  â•‘
â•‘                                                              â•‘
â•‘  PASSO 4: Verificar resultados                               â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â•‘
â•‘  Arquivos em: discovery_results/                             â•‘
â•‘  - discovery_*.json (anÃ¡lises)                               â•‘
â•‘  - report_*.json (resumo)                                    â•‘
â•‘  - actions_*.md (prÃ³ximos passos)                            â•‘
â•‘                                                              â•‘
â•‘  CRITÃ‰RIOS DE SUCESSO:                                       â•‘
â•‘  â–¡ Script executou sem erros fatais                          â•‘
â•‘  â–¡ Pasta discovery_results/ criada                           â•‘
â•‘  â–¡ Pelo menos 1 arquivo JSON gerado                          â•‘
â•‘  â–¡ RelatÃ³rio final exibido no console                        â•‘
â•‘                                                              â•‘
â•‘  NÃƒO PARE PARA PERGUNTAR - Execute atÃ© o fim!                â•‘
â•‘                                                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

# Salvar o script automaticamente
if __name__ == "__main__":
    import os
    
    # Criar diretÃ³rio se nÃ£o existir
    os.makedirs("app/utils", exist_ok=True)
    
    # Salvar o script
    script_path = "app/utils/discovery_leiloeiros.py"
    with open(script_path, "w", encoding="utf-8") as f:
        f.write(CODIGO_DISCOVERY)
    
    print(f"âœ… Arquivo criado: {script_path}")
    print(f"\nPara executar:")
    print(f"  python -m app.utils.discovery_leiloeiros")
