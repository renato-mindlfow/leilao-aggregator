"""
Gera documenta√ß√£o de padr√µes identificados
"""
import os
import sys
import json
from datetime import datetime
import psycopg2
from psycopg2.extras import RealDictCursor

# Adicionar o diret√≥rio raiz ao path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

def gerar_documentacao():
    conn = psycopg2.connect(os.environ.get('DATABASE_URL'))
    cur = conn.cursor(cursor_factory=RealDictCursor)
    
    doc = f"""# üìä Padr√µes de Scraping - LeiloHub

**Gerado em:** {datetime.now().isoformat()}

## Resumo Geral

"""
    
    # Estat√≠sticas gerais
    cur.execute("SELECT COUNT(*) FROM auctioneers")
    total = cur.fetchone()['count']
    
    cur.execute("SELECT COUNT(*) FROM auctioneers WHERE scrape_status = 'success'")
    sucesso = cur.fetchone()['count']
    
    cur.execute("SELECT COUNT(*) FROM auctioneers WHERE scrape_config IS NOT NULL")
    com_config = cur.fetchone()['count']
    
    cur.execute("SELECT COUNT(*) FROM properties")
    imoveis = cur.fetchone()['count']
    
    doc += f"""
| M√©trica | Valor |
|---------|-------|
| Total de leiloeiros | {total} |
| Com scraping funcionando | {sucesso} |
| Com config descoberto | {com_config} |
| Total de im√≥veis | {imoveis} |
| Taxa de sucesso | {sucesso/total*100:.1f}% |

## Padr√µes de Sites

### Tipos de Site Identificados
"""
    
    cur.execute("""
        SELECT 
            scrape_config->>'site_type' as tipo,
            COUNT(*) as count
        FROM auctioneers
        WHERE scrape_config IS NOT NULL
        GROUP BY 1
    """)
    
    for row in cur.fetchall():
        doc += f"- **{row['tipo']}**: {row['count']} sites\n"
    
    doc += """
### Padr√µes de Sucesso

Os sites que funcionam geralmente t√™m:
1. URL direta para listagem de im√≥veis
2. Estrutura HTML consistente
3. Pagina√ß√£o via query parameter (?page=N)
4. Dados estruturados em cards/listas

### Padr√µes de Falha

Os sites que falham geralmente t√™m:
1. Prote√ß√£o Cloudflare agressiva
2. Conte√∫do carregado via JavaScript
3. APIs internas n√£o documentadas
4. Rate limiting

## Erros Comuns

"""
    
    cur.execute("""
        SELECT 
            SUBSTRING(scrape_error, 1, 100) as erro,
            COUNT(*) as count
        FROM auctioneers
        WHERE scrape_error IS NOT NULL
        GROUP BY 1
        ORDER BY 2 DESC
        LIMIT 10
    """)
    
    doc += "| Erro | Ocorr√™ncias |\n|------|-------------|\n"
    for row in cur.fetchall():
        erro = row['erro'] or 'N/A'
        doc += f"| {erro} | {row['count']} |\n"
    
    doc += """
## Recomenda√ß√µes

1. **Para sites list_based**: Usar URL de listagem direta
2. **Para sites filter_based**: Iterar por cada filtro de categoria
3. **Para sites com Cloudflare**: Usar Jina.ai como fallback
4. **Para sites com JavaScript**: Considerar ScrapingBee

## Pr√≥ximos Passos

1. Aumentar cobertura de descoberta (mais leiloeiros)
2. Implementar fallbacks espec√≠ficos por tipo de erro
3. Criar adaptadores customizados para sites problem√°ticos
4. Monitorar taxa de sucesso diariamente
"""
    
    conn.close()
    
    # Salvar
    output_path = os.path.join(os.path.dirname(__file__), '..', 'PADROES_SCRAPING.md')
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(doc)
    
    print(f"Documenta√ß√£o salva em {output_path}")
    print(doc)

if __name__ == "__main__":
    gerar_documentacao()

