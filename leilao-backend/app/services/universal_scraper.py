import os
import re
import json
import httpx
import asyncio
import logging
import traceback
from typing import Dict, List, Optional, Any
from datetime import datetime
from urllib.parse import urljoin, urlparse

logger = logging.getLogger(__name__)


def parse_brazilian_date(date_str: str) -> str | None:
    """
    Converte datas em formato brasileiro para formato PostgreSQL TIMESTAMP.
    
    Exemplos de entrada:
    - "29/12/2025 √†s 14:00"
    - "29/12/2025 14:00"
    - "29/12/2025"
    - "2025-12-29T14:00:00" (j√° est√° em formato v√°lido, ser√° convertido)
    - "2025-12-29 14:00:00" (j√° est√° no formato correto)
    
    Retorna: "2025-12-29 14:00:00" (formato PostgreSQL) ou None se inv√°lido
    """
    if not date_str or not isinstance(date_str, str):
        return None
    
    date_str = date_str.strip()
    
    # Se j√° est√° em formato PostgreSQL (YYYY-MM-DD HH:MM:SS), retornar
    if re.match(r'^\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}', date_str):
        return date_str
    
    # Se est√° em formato ISO (YYYY-MM-DDTHH:MM:SS), converter para PostgreSQL
    if re.match(r'^\d{4}-\d{2}-\d{2}T\d{2}:\d{2}', date_str):
        try:
            dt = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
            return dt.strftime("%Y-%m-%d %H:%M:%S")
        except ValueError:
            pass
    
    # Remover "√†s" e outros textos
    date_str = re.sub(r'\s*√†s\s*', ' ', date_str)
    date_str = re.sub(r'\s+', ' ', date_str).strip()
    
    # Tentar diferentes formatos brasileiros
    formats = [
        "%d/%m/%Y %H:%M",
        "%d/%m/%Y %H:%M:%S",
        "%d/%m/%Y",
        "%d-%m-%Y %H:%M",
        "%d-%m-%Y %H:%M:%S",
        "%d-%m-%Y",
    ]
    
    for fmt in formats:
        try:
            dt = datetime.strptime(date_str, fmt)
            # Retornar no formato PostgreSQL: YYYY-MM-DD HH:MM:SS
            return dt.strftime("%Y-%m-%d %H:%M:%S")
        except ValueError:
            continue
    
    # Se nenhum formato funcionou, retornar None
    return None

OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')
JINA_API_KEY = os.environ.get('JINA_API_KEY', '')  # Opcional

class UniversalScraper:
    """Scraper universal que funciona com qualquer site de leil√£o"""
    
    def __init__(self):
        self.timeout = 30.0
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7',
        }
    
    async def scrape_auctioneer(self, auctioneer: Dict) -> List[Dict]:
        """Scrape um leiloeiro espec√≠fico usando m√∫ltiplas estrat√©gias"""
        
        try:
            if not auctioneer:
                logger.error("Auctioneer √© None")
                return []
            
            website = auctioneer.get('website', '')
            name = auctioneer.get('name', 'Unknown')
            
            if not website:
                logger.error(f"Leiloeiro {name} sem URL")
                return []
            
            logger.info(f"Iniciando scraping de {name}: {website}")
            
            properties = []
            working_url = None  # URL que funcionou
            
            # Tentar encontrar p√°gina de im√≥veis (paths espec√≠ficos primeiro)
            possible_paths = [
                # Paths espec√≠ficos para im√≥veis (prioridade)
                '/lotes/imovel',
                '/lotes/imoveis', 
                '/lotes?tipo=imovel',
                '/lotes?categoria=imovel',
                '/imoveis',
                '/imoveis/todos',
                '/busca?tipo=imovel',
                '/busca?categoria=imovel',
                '/catalogo/imoveis',
                '/leiloes/imoveis',
                '/leiloes',
                # Paths gen√©ricos (√∫ltima op√ß√£o)
                '/catalogo',
                '/',
            ]
            
            for path in possible_paths:
                url = urljoin(website, path)
                
                try:
                    # Estrat√©gia 1: Requisi√ß√£o direta
                    result = await self._try_direct_request(url, name)
                    if result and isinstance(result, list):
                        properties.extend(result)
                        working_url = url  # Salvar URL que funcionou
                        logger.info(f"URL funcionou: {url} - {len(result)} im√≥veis")
                        break
                    
                    # Estrat√©gia 2: Jina Reader (para sites protegidos)
                    result = await self._try_jina_reader(url, name)
                    if result and isinstance(result, list):
                        properties.extend(result)
                        working_url = url  # Salvar URL que funcionou
                        logger.info(f"URL funcionou via Jina: {url} - {len(result)} im√≥veis")
                        break
                        
                except Exception as e:
                    logger.warning(f"Erro ao tentar {url}: {e}")
                    logger.debug(traceback.format_exc())
                    continue
            
            # Pagina√ß√£o: tentar buscar mais p√°ginas usando a URL que funcionou
            if properties and working_url:
                logger.info(f"Iniciando pagina√ß√£o para {name}...")
                try:
                    more_properties = await self._scrape_pagination(working_url, name, len(properties))
                    if more_properties and isinstance(more_properties, list):
                        properties.extend(more_properties)
                    logger.info(f"Ap√≥s pagina√ß√£o: {len(properties)} im√≥veis total")
                except Exception as e:
                    logger.warning(f"Erro na pagina√ß√£o de {name}: {e}")
                    logger.debug(traceback.format_exc())
            
            logger.info(f"Scraping de {name} finalizado: {len(properties)} im√≥veis encontrados")
            
            return properties
            
        except Exception as e:
            logger.error(f"Erro no scraping: {e}")
            logger.error(traceback.format_exc())
            return []
    
    async def _try_direct_request(self, url: str, source: str) -> Optional[List[Dict]]:
        """Tenta requisi√ß√£o HTTP direta"""
        
        try:
            async with httpx.AsyncClient(timeout=self.timeout, follow_redirects=True) as client:
                response = await client.get(url, headers=self.headers)
                
                if not response:
                    logger.warning(f"Resposta vazia de {url}")
                    return None
                
                if response.status_code != 200:
                    logger.debug(f"Status code {response.status_code} de {url}")
                    return None
                
                content_type = response.headers.get('content-type', '')
                
                # Se for JSON, processar diretamente
                if 'application/json' in content_type:
                    try:
                        data = response.json()
                        if data is None:
                            logger.warning(f"JSON vazio de {url}")
                            return None
                        return self._parse_json_response(data, source, url)
                    except json.JSONDecodeError as e:
                        logger.warning(f"Erro ao parsear JSON de {url}: {e}")
                        return None
                
                # Se for HTML, usar IA para extrair
                html = response.text
                
                if not html:
                    logger.warning(f"HTML vazio de {url}")
                    return None
                
                # Verificar se tem conte√∫do de im√≥veis
                if not self._has_property_content(html):
                    return None
                
                return await self._extract_with_ai(html, source, url)
                
        except Exception as e:
            logger.debug(f"Requisi√ß√£o direta falhou para {url}: {e}")
            logger.debug(traceback.format_exc())
            return None
    
    async def _try_jina_reader(self, url: str, source: str) -> Optional[List[Dict]]:
        """Usa Jina Reader para sites protegidos por Cloudflare"""
        
        if not JINA_API_KEY:
            # Jina Reader gratuito (sem API key)
            jina_url = f"https://r.jina.ai/{url}"
        else:
            jina_url = f"https://r.jina.ai/{url}"
        
        try:
            async with httpx.AsyncClient(timeout=60.0) as client:
                headers = {'Accept': 'text/plain'}
                if JINA_API_KEY:
                    headers['Authorization'] = f'Bearer {JINA_API_KEY}'
                
                response = await client.get(jina_url, headers=headers)
                
                if not response:
                    logger.warning(f"Resposta vazia do Jina Reader para {url}")
                    return None
                
                if response.status_code != 200:
                    logger.debug(f"Jina Reader retornou status {response.status_code} para {url}")
                    return None
                
                text = response.text
                
                if not text:
                    logger.warning(f"Texto vazio do Jina Reader para {url}")
                    return None
                
                if not self._has_property_content(text):
                    return None
                
                return await self._extract_with_ai(text, source, url, is_markdown=True)
                
        except Exception as e:
            logger.debug(f"Jina Reader falhou para {url}: {e}")
            logger.debug(traceback.format_exc())
            return None
    
    def _has_property_content(self, content: str) -> bool:
        """Verifica se o conte√∫do parece ter im√≥veis"""
        
        keywords = [
            'apartamento', 'casa', 'terreno', 'im√≥vel', 'imovel',
            'leil√£o', 'leilao', 'lance', 'avalia√ß√£o', 'avaliacao',
            'r$', 'reais', 'desconto', 'arremata√ß√£o',
            'm¬≤', 'm2', 'metros', 'quartos', 'dormit√≥rios'
        ]
        
        content_lower = content.lower()
        matches = sum(1 for kw in keywords if kw in content_lower)
        
        return matches >= 3
    
    def _filter_properties_by_category(self, properties: List[Dict], source: str) -> List[Dict]:
        """Filtra propriedades para incluir APENAS im√≥veis (exclui ve√≠culos, m√°quinas, etc)"""
        
        CATEGORIAS_IMOVEIS = [
            'imovel', 'im√≥vel', 'im√≥veis', 'imoveis',
            'apartamento', 'casa', 'terreno', 'comercial',
            'rural', 'galp√£o', 'galpao', 'sala', 'loja',
            'fazenda', 's√≠tio', 'sitio', 'ch√°cara', 'chacara',
            'pr√©dio', 'predio', 'edificio', 'edif√≠cio',
        ]
        
        NAO_IMOVEL = [
            've√≠culo', 'veiculo', 'carro', 'moto', 'caminh√£o', 'caminhao',
            'm√°quina', 'maquina', 'equipamento', 'm√≥vel', 'movel',
            'eletr√¥nico', 'eletronico', 'j√≥ia', 'joia', 'joia', 'j√≥ias',
            'animal', 'trator', 'colheitadeira'
        ]
        
        filtered = []
        for prop in properties:
            cat = (prop.get('category', '') or '').lower()
            titulo = (prop.get('title', '') or '').lower()
            
            # Verificar se √© im√≥vel
            is_imovel = any(c in cat for c in CATEGORIAS_IMOVEIS) or \
                        any(c in titulo for c in CATEGORIAS_IMOVEIS)
            
            # Rejeitar se claramente N√ÉO √© im√≥vel
            is_not_imovel = any(n in cat for n in NAO_IMOVEL) or \
                            any(n in titulo for n in NAO_IMOVEL)
            
            if is_imovel or (not is_not_imovel and not cat):
                # Se n√£o tem categoria definida e n√£o parece ser N√ÉO-im√≥vel, incluir
                filtered.append(prop)
            else:
                logger.debug(f"Filtrado (n√£o √© im√≥vel): {prop.get('title', 'N/A')[:50]}")
        
        return filtered
    
    def _parse_json_response(self, data: Any, source: str, url: str) -> List[Dict]:
        """Parseia resposta JSON de APIs"""
        
        properties = []
        
        if not data:
            logger.warning(f"JSON vazio de {url}")
            return []
        
        # Tentar encontrar array de im√≥veis em diferentes estruturas
        items = []
        
        if isinstance(data, list):
            items = data
        elif isinstance(data, dict):
            # Procurar em campos comuns
            for key in ['data', 'items', 'results', 'imoveis', 'leiloes', 'properties', 'lotes']:
                if key in data and isinstance(data[key], list):
                    items = data[key]
                    break
        
        if not items:
            logger.debug(f"Nenhum item encontrado no JSON de {url}")
            return []
        
        for item in items:
            if not item or not isinstance(item, dict):
                continue
            
            prop = self._normalize_json_item(item, source, url)
            if prop:
                properties.append(prop)
        
        # Filtrar apenas im√≥veis
        filtered = self._filter_properties_by_category(properties, source)
        logger.info(f"Filtro de categoria (JSON): {len(properties)} -> {len(filtered)} im√≥veis")
        
        return filtered
    
    def _normalize_json_item(self, item: Dict, source: str, base_url: str) -> Optional[Dict]:
        """Normaliza um item JSON para o formato padr√£o"""
        
        # Mapear campos comuns
        field_mappings = {
            'title': ['titulo', 'title', 'nome', 'name', 'descricao_resumida', 'description'],
            'description': ['descricao', 'description', 'detalhes', 'details', 'observacao'],
            'price': ['valor', 'price', 'preco', 'lance_inicial', 'valor_minimo', 'valor_avaliacao', 'first_auction_value'],
            'evaluated_price': ['valor_avaliacao', 'avaliacao', 'valor_mercado', 'evaluation', 'evaluation_value'],
            'discount': ['desconto', 'discount', 'percentual_desconto', 'discount_percentage'],
            'address': ['endereco', 'address', 'localizacao', 'location', 'logradouro'],
            'city': ['cidade', 'city', 'municipio'],
            'state': ['estado', 'state', 'uf'],
            'category': ['tipo', 'category', 'categoria', 'tipo_imovel', 'property_type'],
            'area': ['area', 'metragem', 'area_total', 'area_privativa', 'm2'],
            'image_url': ['imagem', 'image', 'foto', 'photo', 'thumbnail', 'imagem_url', 'image_url'],
            'url': ['url', 'link', 'href', 'detalhes_url', 'source_url'],
            'auction_date': ['data_leilao', 'auction_date', 'data', 'date', 'data_praca', 'first_auction_date'],
            'external_id': ['id', 'codigo', 'code', 'ref', 'referencia', 'lote'],
        }
        
        prop = {'source': source}
        
        for standard_field, possible_fields in field_mappings.items():
            for field in possible_fields:
                if field in item and item[field]:
                    prop[standard_field] = item[field]
                    break
        
        # Garantir que tem pelo menos t√≠tulo ou ID
        if not prop.get('title') and not prop.get('external_id'):
            return None
        
        # Ajustar URL da imagem
        if prop.get('image_url') and not prop['image_url'].startswith('http'):
            prop['image_url'] = urljoin(base_url, prop['image_url'])
        
        # Ajustar URL do im√≥vel
        if prop.get('url') and not prop['url'].startswith('http'):
            prop['url'] = urljoin(base_url, prop['url'])
        
        # Criar external_id se n√£o existir
        if not prop.get('external_id'):
            prop['external_id'] = f"{source}_{hash(str(item))}"
        else:
            prop['external_id'] = f"{source}_{prop['external_id']}"
        
        return prop
    
    async def _extract_with_ai(self, content: str, source: str, url: str, is_markdown: bool = False) -> List[Dict]:
        """Usa IA para extrair im√≥veis do HTML/texto"""
        
        if not content:
            logger.warning(f"Conte√∫do vazio para extra√ß√£o com IA de {url}")
            return []
        
        if not OPENAI_API_KEY:
            logger.warning("OPENAI_API_KEY n√£o configurada, pulando extra√ß√£o com IA")
            return []
        
        # Limitar tamanho do conte√∫do
        content = content[:50000]
        
        prompt = f"""Analise o conte√∫do abaixo de um site de leil√£o de im√≥veis e extraia TODOS os im√≥veis listados.

Conte√∫do do site ({source}):
{content[:30000]}

Para cada im√≥vel encontrado, extraia:
- title: t√≠tulo ou descri√ß√£o curta
- price: valor do lance/leil√£o (n√∫mero)
- evaluated_price: valor de avalia√ß√£o (n√∫mero)
- discount: percentual de desconto (n√∫mero)
- address: endere√ßo completo
- city: cidade
- state: estado (sigla UF)
- category: tipo (Apartamento, Casa, Terreno, Comercial, etc)
- area: √°rea em m¬≤ (n√∫mero)
- image_url: URL da imagem
- url: URL da p√°gina do im√≥vel
- auction_date: data do leil√£o
- external_id: c√≥digo/refer√™ncia do lote

Retorne APENAS um JSON array v√°lido com os im√≥veis. Se n√£o encontrar im√≥veis, retorne [].
Exemplo: [{{"title": "...", "price": 100000, ...}}]"""

        try:
            async with httpx.AsyncClient(timeout=60.0) as client:
                response = await client.post(
                    "https://api.openai.com/v1/chat/completions",
                    headers={
                        "Authorization": f"Bearer {OPENAI_API_KEY}",
                        "Content-Type": "application/json"
                    },
                    json={
                        "model": "gpt-4o-mini",
                        "messages": [{"role": "user", "content": prompt}],
                        "temperature": 0.1,
                        "max_tokens": 4000
                    }
                )
                
                if response.status_code != 200:
                    logger.error(f"Erro na API OpenAI: {response.text}")
                    return []
                
                result = response.json()
                
                # Verificar se a resposta √© v√°lida
                if not result or not result.get('choices'):
                    logger.error("Resposta vazia da OpenAI ou sem choices")
                    return []
                
                if not result['choices'] or len(result['choices']) == 0:
                    logger.error("Lista de choices vazia da OpenAI")
                    return []
                
                message = result['choices'][0].get('message')
                if not message:
                    logger.error("Mensagem vazia na resposta da OpenAI")
                    return []
                
                content = message.get('content')
                if not content:
                    logger.error("Conte√∫do vazio na resposta da OpenAI")
                    return []
                
                # Limpar resposta
                content = content.strip()
                if content.startswith('```json'):
                    content = content[7:]
                if content.startswith('```'):
                    content = content[3:]
                if content.endswith('```'):
                    content = content[:-3]
                content = content.strip()
                
                items = json.loads(content)
                
                if not isinstance(items, list):
                    return []
                
                # Adicionar source e ajustar IDs
                properties = []
                for item in items:
                    if isinstance(item, dict):
                        item['source'] = source
                        if not item.get('external_id'):
                            item['external_id'] = f"{source}_{hash(str(item))}"
                        else:
                            item['external_id'] = f"{source}_{item['external_id']}"
                        
                        # Ajustar URLs
                        if item.get('image_url') and not item['image_url'].startswith('http'):
                            item['image_url'] = urljoin(url, item['image_url'])
                        if item.get('url') and not item['url'].startswith('http'):
                            item['url'] = urljoin(url, item['url'])
                        
                        properties.append(item)
                
                # Filtrar apenas im√≥veis (evitar ve√≠culos, m√°quinas, etc)
                filtered = self._filter_properties_by_category(properties, source)
                logger.info(f"Filtro de categoria: {len(properties)} -> {len(filtered)} im√≥veis")
                
                return filtered
                
        except json.JSONDecodeError as e:
            logger.warning(f"Erro ao parsear JSON da IA: {e}")
            logger.debug(traceback.format_exc())
            return []
        except Exception as e:
            logger.error(f"Erro na extra√ß√£o com IA: {e}")
            logger.error(traceback.format_exc())
            return []
    
    async def _scrape_pagination(self, working_url: str, source: str, initial_count: int) -> List[Dict]:
        """Tenta buscar p√°ginas adicionais a partir da URL que funcionou"""
        
        additional = []
        seen_urls = set()  # Evitar loop infinito
        
        # Padr√µes comuns de pagina√ß√£o
        pagination_patterns = [
            '?page={page}',
            '?pagina={page}',
            '?p={page}',
            '&page={page}',
            '&pagina={page}',
            '&p={page}',
        ]
        
        # Verificar se a URL j√° tem query string
        has_query = '?' in working_url
        
        logger.info(f"Iniciando pagina√ß√£o para {source} a partir de {working_url}")
        
        for pattern in pagination_patterns:
            # Ajustar padr√£o se URL j√° tem query string
            if has_query and pattern.startswith('?'):
                pattern = pattern.replace('?', '&')
            
            consecutive_empty = 0  # Contador de p√°ginas vazias consecutivas
            
            for page in range(2, 51):
                url = working_url.rstrip('/') + pattern.format(page=page)
                
                # Evitar URLs j√° visitadas
                if url in seen_urls:
                    logger.info(f"URL j√° visitada, parando: {url}")
                    break
                seen_urls.add(url)
                
                logger.debug(f"Tentando p√°gina {page}: {url}")
                
                try:
                    result = await self._try_direct_request(url, source)
                    
                    if not result or len(result) == 0:
                        consecutive_empty += 1
                        if consecutive_empty >= 2:  # 2 p√°ginas vazias = parar
                            logger.info(f"2 p√°ginas vazias consecutivas, parando pagina√ß√£o")
                            break
                        continue
                    
                    consecutive_empty = 0  # Reset contador
                    
                    # Verificar se s√£o os mesmos itens da p√°gina anterior (loop)
                    new_titles = set(p.get('title', '') for p in result)
                    existing_titles = set(p.get('title', '') for p in additional[-20:] if additional)
                    
                    overlap = len(new_titles & existing_titles)
                    if overlap > len(new_titles) * 0.8:  # 80% repetidos = loop
                        logger.warning(f"Detectado loop de conte√∫do na p√°gina {page}, parando")
                        break
                    
                    additional.extend(result)
                    logger.info(f"P√°gina {page} de {source}: +{len(result)} im√≥veis (total acumulado: {initial_count + len(additional)})")
                    
                    # Pequeno delay para n√£o sobrecarregar
                    await asyncio.sleep(0.5)
                        
                except Exception as e:
                    logger.warning(f"Erro na p√°gina {page} de {source}: {e}")
                    break
            
            # Se encontrou resultados com este padr√£o, n√£o tentar outros
            if additional:
                logger.info(f"Pagina√ß√£o de {source} completa: +{len(additional)} im√≥veis adicionais")
                break
        
        return additional
    
    async def scrape_with_config(self, auctioneer: Dict, config: Dict) -> List[Dict]:
        """
        Executa scraping usando configura√ß√£o descoberta.
        
        Args:
            auctioneer: Dict com dados do leiloeiro
            config: Dict com configura√ß√£o descoberta (scrape_config)
            
        Returns:
            Lista de im√≥veis extra√≠dos
        """
        website = auctioneer.get("website", "").rstrip("/")
        name = auctioneer.get("name", "Unknown")
        site_type = config.get("site_type", "unknown")
        
        logger.info(f"üéØ Scraping {name} usando configura√ß√£o ({site_type})")
        
        all_properties = []
        
        # ESTRAT√âGIA 1: Site com filtros
        if site_type == "filter_based" and config.get("property_filters"):
            logger.info(f"Usando {len(config['property_filters'])} filtros de im√≥veis")
            
            for filter_info in config["property_filters"]:
                filter_name = filter_info.get("name", "Unknown")
                filter_url = filter_info.get("url")
                
                if not filter_url:
                    continue
                
                logger.info(f"  ‚Üí Filtro: {filter_name} ({filter_url})")
                
                # Extrair im√≥veis deste filtro
                properties = await self._extract_from_url(filter_url, name)
                
                if properties:
                    # Adicionar categoria baseada no filtro
                    for prop in properties:
                        if not prop.get("category"):
                            prop["category"] = filter_name
                    
                    all_properties.extend(properties)
                    logger.info(f"    ‚úì {len(properties)} im√≥veis de {filter_name}")
                
                # Pagina√ß√£o para este filtro
                if config.get("pagination") and len(properties) >= 10:
                    pagination_props = await self._paginate_with_config(
                        filter_url, name, config["pagination"]
                    )
                    all_properties.extend(pagination_props)
        
        # ESTRAT√âGIA 2: Site com lista √∫nica
        elif site_type == "list_based" or config.get("fallback_url"):
            fallback_url = config.get("fallback_url", website)
            logger.info(f"Usando URL direta: {fallback_url}")
            
            properties = await self._extract_from_url(fallback_url, name)
            
            if properties:
                all_properties.extend(properties)
                
                # Pagina√ß√£o
                if config.get("pagination"):
                    pagination_props = await self._paginate_with_config(
                        fallback_url, name, config["pagination"]
                    )
                    all_properties.extend(pagination_props)
        
        # ESTRAT√âGIA 3: Fallback para m√©todo antigo
        else:
            logger.warning(f"Configura√ß√£o incompleta, usando m√©todo tradicional")
            return await self.scrape_auctioneer(auctioneer)
        
        # Deduplicar por URL
        seen_urls = set()
        unique_properties = []
        for prop in all_properties:
            url = prop.get("source_url") or prop.get("url")
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_properties.append(prop)
        
        logger.info(f"‚úÖ {name}: {len(unique_properties)} im√≥veis √∫nicos extra√≠dos")
        
        return unique_properties
    
    async def _extract_from_url(self, url: str, source: str) -> List[Dict]:
        """Extrai im√≥veis de uma URL espec√≠fica"""
        
        try:
            # Tentar fetch direto
            html = None
            async with httpx.AsyncClient(timeout=30.0, follow_redirects=True) as client:
                response = await client.get(url, headers={
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
                })
                if response.status_code == 200 and len(response.text) > 1000:
                    html = response.text
            
            # Fallback para Jina
            if not html:
                async with httpx.AsyncClient(timeout=60.0) as client:
                    jina_url = f"https://r.jina.ai/{url}"
                    response = await client.get(jina_url, headers={"X-Return-Format": "html"})
                    if response.status_code == 200:
                        html = response.text
            
            if not html:
                return []
            
            # Extrair com IA
            properties = await self._extract_with_ai(html, source, url)
            
            # Aplicar filtro de categoria
            properties = self._filter_properties_by_category(properties, source)
            
            return properties
            
        except Exception as e:
            logger.error(f"Erro ao extrair de {url}: {e}")
            return []
    
    async def _paginate_with_config(self, base_url: str, source: str, pagination_config: Dict) -> List[Dict]:
        """Pagina usando configura√ß√£o descoberta"""
        
        additional = []
        pagination_type = pagination_config.get("type", "query_param")
        param = pagination_config.get("param", "page")
        pattern = pagination_config.get("pattern", f"?{param}={{n}}")
        start = pagination_config.get("start", 1)
        
        consecutive_empty = 0
        
        for page in range(start + 1, 51):  # M√°ximo 50 p√°ginas
            # Construir URL da p√°gina
            if pagination_type == "query_param":
                if "?" in base_url:
                    page_url = f"{base_url}&{param}={page}"
                else:
                    page_url = f"{base_url}?{param}={page}"
            elif pagination_type == "path_segment":
                page_url = f"{base_url.rstrip('/')}/page/{page}"
            else:
                page_url = base_url + pattern.replace("{n}", str(page))
            
            properties = await self._extract_from_url(page_url, source)
            
            if not properties:
                consecutive_empty += 1
                if consecutive_empty >= 2:
                    logger.info(f"2 p√°ginas vazias, parando pagina√ß√£o")
                    break
                continue
            
            consecutive_empty = 0
            additional.extend(properties)
            logger.info(f"P√°gina {page}: +{len(properties)} im√≥veis")
        
        return additional


# Inst√¢ncia global
universal_scraper = UniversalScraper()

