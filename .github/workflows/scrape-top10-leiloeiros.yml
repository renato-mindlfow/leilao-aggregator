# .github/workflows/scrape-top10-leiloeiros.yml
# 
# Workflow para scraping automatizado dos TOP 10 leiloeiros do concorrente
# + Universal Scraper para os demais 481 leiloeiros
#
# Executa 2x ao dia: 6h e 18h (horÃ¡rio de BrasÃ­lia)

name: Scrape TOP 10 Leiloeiros + Universal

on:
  # ExecuÃ§Ã£o agendada
  schedule:
    # 6:00 BRT (09:00 UTC)
    - cron: '0 9 * * *'
    # 18:00 BRT (21:00 UTC)  
    - cron: '0 21 * * *'
  
  # ExecuÃ§Ã£o manual
  workflow_dispatch:
    inputs:
      mode:
        description: 'Modo de execuÃ§Ã£o'
        required: true
        default: 'all'
        type: choice
        options:
          - all          # TOP 10 + Universal
          - top10        # Apenas TOP 10
          - universal    # Apenas Universal Scraper
      max_properties:
        description: 'MÃ¡ximo de propriedades por leiloeiro'
        required: false
        default: '100'
        type: string
      limit_top10:
        description: 'Limite de leiloeiros TOP 10 (1-10)'
        required: false
        default: '10'
        type: string

env:
  DATABASE_URL: ${{ secrets.DATABASE_URL }}
  PYTHONUNBUFFERED: 1

jobs:
  scrape-top10:
    name: ðŸ† Scrape TOP 10 Leiloeiros
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.mode != 'universal' }}
    timeout-minutes: 60
    
    steps:
      - name: ðŸ“¥ Checkout cÃ³digo
        uses: actions/checkout@v4
        
      - name: ðŸ Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          
      - name: ðŸ“¦ Instalar dependÃªncias
        run: |
          pip install --upgrade pip
          pip install playwright psycopg2-binary python-dotenv
          playwright install chromium
          playwright install-deps chromium
          
      - name: ðŸ” Verificar script
        run: |
          ls -la scripts/
          head -50 scripts/SCRAPER_TOP10_CORRIGIDO.py
          
      - name: ðŸš€ Executar Scraper TOP 10
        run: |
          cd scripts
          python SCRAPER_TOP10_CORRIGIDO.py \
            --limit ${{ github.event.inputs.limit_top10 || '10' }} \
            --max-properties ${{ github.event.inputs.max_properties || '100' }} \
            --headless true
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
          
      - name: ðŸ“Š Upload resultados
        uses: actions/upload-artifact@v4
        with:
          name: top10-results-${{ github.run_id }}
          path: scripts/top10_scraping_*.json
          retention-days: 7
          
      - name: ðŸ“ Resumo
        run: |
          echo "## ðŸ† Scraping TOP 10 ConcluÃ­do" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          if [ -f scripts/top10_scraping_*.json ]; then
            echo "Arquivo de resultados gerado com sucesso!" >> $GITHUB_STEP_SUMMARY
            TOTAL=$(cat scripts/top10_scraping_*.json | python3 -c "import json,sys; data=json.load(sys.stdin); print(data.get('total_properties', 0))")
            echo "- **Total de propriedades:** $TOTAL" >> $GITHUB_STEP_SUMMARY
          fi

  scrape-universal:
    name: ðŸŒ Universal Scraper
    runs-on: ubuntu-latest
    needs: [scrape-top10]
    if: ${{ always() && (github.event.inputs.mode != 'top10') }}
    timeout-minutes: 120
    
    steps:
      - name: ðŸ“¥ Checkout cÃ³digo
        uses: actions/checkout@v4
        
      - name: ðŸ Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          
      - name: ðŸ“¦ Instalar dependÃªncias
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt || pip install playwright psycopg2-binary python-dotenv httpx beautifulsoup4
          
      - name: ðŸš€ Executar Universal Scraper
        run: |
          # Verificar se existe o script universal
          if [ -f "app/services/universal_scraper_service.py" ]; then
            python -c "
from app.services.universal_scraper_service import UniversalScraperService
import asyncio
import os

async def main():
    service = UniversalScraperService()
    # Processar leiloeiros com status 'pending' ou que falharam
    results = await service.scrape_all_auctioneers(
        max_auctioneers=50,
        max_properties_per_auctioneer=50
    )
    print(f'Processados: {len(results)} leiloeiros')

asyncio.run(main())
"
          else
            echo "Universal Scraper nÃ£o encontrado, pulando..."
          fi
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
          
      - name: ðŸ“ Resumo Universal
        run: |
          echo "## ðŸŒ Universal Scraper ConcluÃ­do" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Scraping dos demais leiloeiros executado." >> $GITHUB_STEP_SUMMARY

  sync-caixa:
    name: ðŸ¦ Sync Caixa EconÃ´mica
    runs-on: ubuntu-latest
    needs: [scrape-universal]
    if: ${{ always() }}
    timeout-minutes: 30
    
    steps:
      - name: ðŸ“¥ Checkout cÃ³digo
        uses: actions/checkout@v4
        
      - name: ðŸ Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          
      - name: ðŸ“¦ Instalar dependÃªncias
        run: |
          pip install httpx psycopg2-binary python-dotenv
          
      - name: ðŸ¦ Sync Caixa
        run: |
          # Chamar API de sync da Caixa (se existir endpoint)
          if [ -f "scripts/sync_caixa.py" ]; then
            python scripts/sync_caixa.py
          else
            echo "Script de sync Caixa nÃ£o encontrado"
          fi
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
          
  report:
    name: ðŸ“Š Gerar RelatÃ³rio
    runs-on: ubuntu-latest
    needs: [scrape-top10, scrape-universal, sync-caixa]
    if: ${{ always() }}
    
    steps:
      - name: ðŸ“¥ Download artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: '*-results-*'
          merge-multiple: true
          path: results/
        continue-on-error: true
        
      - name: ðŸ“Š Gerar relatÃ³rio final
        run: |
          echo "## ðŸ“Š RelatÃ³rio de Scraping - $(date +'%Y-%m-%d %H:%M')" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "### Jobs Executados" >> $GITHUB_STEP_SUMMARY
          echo "| Job | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-----|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| TOP 10 Leiloeiros | ${{ needs.scrape-top10.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Universal Scraper | ${{ needs.scrape-universal.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Sync Caixa | ${{ needs.sync-caixa.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -d "results" ] && [ "$(ls -A results 2>/dev/null)" ]; then
            echo "### Arquivos Gerados" >> $GITHUB_STEP_SUMMARY
            ls -la results/ >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "---" >> $GITHUB_STEP_SUMMARY
          echo "Workflow: ${{ github.workflow }}" >> $GITHUB_STEP_SUMMARY
          echo "Run ID: ${{ github.run_id }}" >> $GITHUB_STEP_SUMMARY
